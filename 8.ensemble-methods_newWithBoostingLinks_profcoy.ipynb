{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of ensemble-methods.ipynb","version":"0.3.2","provenance":[{"file_id":"1HwsPmUSN841wqE7oJpQ-ekZcc_TnFM8Y","timestamp":1551205211583}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"FmQhJP3L1vUh","colab_type":"text"},"cell_type":"markdown","source":["\n","# Bootstrapping, Bagging, Random Forests, and ExtraTrees"]},{"metadata":{"id":"5Mi_ae4u1vUi","colab_type":"text"},"cell_type":"markdown","source":["## Introduction to Ensemble Methods\n","We can list out the different types of models we've built thus far:\n","- Linear Regression\n","- Generalized Linear Models\n","    - Logistic Regression\n","    - Multinomial Logistic Regression\n","    - Poisson Regression\n","- $k$-Nearest Neighbors\n","- Naive Bayes Classification\n","\n","If we want to use any of these models, we follow the same type of process.\n","1. Based on our problem, we identify which model to use. (Is our problem classification or regression? Do we want an interpretable model?)\n","2. Fit the model using the training data.\n","3. Use the fit model to generate predictions.\n","4. Evaluate our model's performance and, if necessary, return to step 2 and make changes.\n","\n","In this case, we've always had exactly one model. Today, however, we're going to talk about **ensemble methods**. Mentally, you should think about this as if we build multiple models and then aggregate their results in some way.\n","\n","### Why would we build an \"ensemble model?\"\n","Consider $H$ the space of all possible hypotheses. Our goal is to estimate $f$, the true function. We can come up with different hypotheses $h_1$, $h_2$, and so on to get as close to $f$ as possible. \n","- Think about $f$ as the true process that dictates Iowa liquor sales.\n","- Think about $h_1$ as the model you built to predict $f$.\n","\n","\n","- The **statistical** benefit to ensemble methods: By building one model, our predictions are almost certainly going to be wrong. Predictions from one model might overestimate liquor sales; predictions from another model might underestimate liquor sales. By \"averaging\" predictions from multiple models, we'll see that we can often cancel our errors out and get closer to the true function $f$.\n","- The **computational** benefit to ensemble methods: It might be impossible to develop one model that globally optimizes our objective function. (Remember that CART reach locally-optimal solutions and that all generalized linear models iterate toward a solution that isn't guaranteed to be the globally-optimal solution.) In these cases, it may be impossible for one CART or an individual GLM to arrive at the true function $f$. However, starting our \"local searches\" at different points and aggregating our predictions might .\n","- The **representational** benefit to ensemble methods: Even if we had all the data and all the computer power in the world, ot might be impossible for one model to exactly equal $f$. For example, a linear regression model can never model a relationship where a one-unit change in $X$ effects some *different* change in $Y$ based on the value of $X$. All models have some shortcomings. (See [the no free lunch theorems](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization).) While individual models have shortcomings, by creating multiple models and aggregating their predictions, we can actually create predictions that represent something that one model cannot ever represent."]},{"metadata":{"id":"uGQPf46G1vUj","colab_type":"text"},"cell_type":"markdown","source":["## Bootstrapping\n","\n","Let's get started actually making ensemble predictions. However, in order to do that, we'll need to introduce the idea of bootstrapping, or random sampling **with replacement.**\n","\n","---\n","\n"]},{"metadata":{"collapsed":true,"id":"lfvj_Wx71vUk","colab_type":"code","outputId":"afe8da41-24be-4b08-a188-5afb79e222ef","executionInfo":{"status":"ok","timestamp":1551160898239,"user_tz":480,"elapsed":766,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# scikit-learn bootstrap\n","from sklearn.utils import resample\n","# data sample\n","data = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n","# prepare bootstrap sample\n","boot = resample(data, replace=True, n_samples=4)\n","print('Bootstrap Sample: %s' % boot)\n","# out of bag observations\n","oob = [x for x in data if x not in boot]\n","print('OOB Sample: %s' % oob)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Bootstrap Sample: [0.1, 0.1, 0.6, 0.1]\n","OOB Sample: [0.2, 0.3, 0.4, 0.5]\n"],"name":"stdout"}]},{"metadata":{"id":"dSdKkZUe1vUo","colab_type":"text"},"cell_type":"markdown","source":["## Bagged Decision Trees\n","\n","As we have seen, decision trees are very powerful machine learning models. However, decision trees have some limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns (a.k.a. they overfit their training sets). \n","\n","\n","Bagging (bootstrap aggregating) helps to mitigate this problem by exposing different trees to different sub-samples of the whole training set.\n","\n","The process for creating bagged decision trees is as follows:\n","1. From the original data of size $n$, bootstrap $k$ samples each of size $n$ (with replacement!).\n","2. Build a decision tree on each bootstrapped sample.\n","3. Make predictions by passing a test observation through all $k$ trees and developing one aggregate prediction for that observation.\n","\n","<a href='http://www.cse.buffalo.edu/~jing/sdm10ensemble.htm'>\n","    <img src='https://drive.google.com/uc?export=view&id=1SS4CZSjKxCycLD28AO0I2pm-6lvdWU1a'/>\n","</a>\n","\n","### What do you mean by \"aggregate prediction?\"\n","As with all of our modeling techniques, we want to make sure that we can come up with one final prediction for our observation. (Building 1,000 trees and coming up with 1,000 predictions for one observation probably wouldn't be very helpful.)\n","\n","Suppose we want to predict whether or not a Reddit post is going to go viral, where `1` indicates viral and `0` indicates non-viral. We build 100 decision trees. Given a new Reddit post labeled `X_test`, we pass these features into all 100 decision trees.\n","- 70 of the trees predict that the post in `X_test` will go viral.\n","- 30 of the trees predict that the post in `X_test` will not go viral.\n","\n","<details><summary> What might you expect `.predict(X_test)` and `.predict_proba(X_test)` to do?\n","</summary>\n","```\n","- .predict(X_test) should output a 1, predicting that the post will go viral.\n","- .predict_proba(X_test) should output 0.7, indicating the probability of the post going viral is 70%.\n","```\n","</details>\n","\n","- **Discrete:** In ensemble methods, we will most commonly predict a discrete $Y$ by \"plurality vote,\" where the most common class is the predicted value for a given observation.\n","- **Continuous:** In ensemble methods, we will most commonly predict a continuous $Y$ by averaging the predicted values into one final prediction.\n","\n","---\n","\n","## Random Forests\n","\n","With bagged decision trees, we generate many different trees on pretty similar data. These trees are **strongly correlated** with one another. Because these trees are correlated with one another, they will have high variance. Looking at the variance of the average of two random variables $T_1$ and $T_2$:\n","\n","$$\n","\\begin{eqnarray*}\n","Var\\left(\\frac{T_1+T_2}{2}\\right) &=& \\frac{1}{4}\\left[Var(T_1) + Var(T_2) + 2Cov(T_1,T_2)\\right]\n","\\end{eqnarray*}\n","$$\n","\n","If $T_1$ and $T_2$ are highly correlated, then the variance will about as high as we'd see with individual decision trees. By \"decorrelating\" our trees from one another, we can drastically reduce the variance of our model.\n","\n","That's the difference between bagged decision trees and random forests! We're going to do the same thing as before, but we're going to decorrelate our trees. This will reduce our variance (at the expense of a small increase in bias) and thus should greatly improve the overall performance of the final model.\n","\n","### So how do we \"decorrelate\" our trees?\n","Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each split in the learning process, a **random subset of the features**. This process is sometimes called the *random subspace method*.\n","\n","The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be used in many/all of the bagged decision trees, causing them to become correlated. By selecting a random subset of the features at each split, we counter this correlation between base trees, strengthening the overall model.\n","\n","For a problem with $p$ features, it is typical to use:\n","\n","- $\\sqrt{p}$ (rounded down) features in each split for a classification problem.\n","- $p/3$ (rounded down) with a minimum node size of 5 as the default for a regression problem.\n","\n","While this is a guideline, Hastie and Tibshirani (authors of Introduction to Statistical Learning and Elements of Statistical Learning) have suggested this as a good rule in the absence of some rationale to do something different.\n","\n","Random forests, a step beyond bagged decision trees, are **very widely used** classifiers and regressors. They are relatively simple to use because they require very few parameters to set and they perform pretty well.\n","- It is quite common for interviewers to ask how a random forest is constructed or how it is superior to a single decision tree.\n","\n","--- \n","\n","## Extremely Randomized Trees (ExtraTrees)\n","Adding one more step of randomization (and thus decorrelation) yields extremely randomized trees, or _ExtraTrees_. These are trained using bagging (sampling of observations) and the random subspace method (sampling of features), but an additional layer of randomness is introduced. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity) for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range.\n","\n","This further reduces the variance, but causes an increase in bias. If you're considering using ExtraTrees, you might consider this to be a hyperparameter you can tune. Build an ExtraTrees model, a Random Forest model, and a Bagged Decision Treesmodel, then compare their performance!\n","\n","That's exactly what we'll do below."]},{"metadata":{"id":"-eJx5o3z1vUo","colab_type":"text"},"cell_type":"markdown","source":["## Guided Practice: Random Forest and ExtraTrees in Scikit Learn\n","\n","Scikit Learn implements both random forest and extra trees methods as part of the `ensemble` module.\n","\n","Have a look at the [documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest).\n","\n","**Check:** What parameters did you notice? Any questions on those?\n","\n","Let's load the [car dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/).\n","\n","We would like to compare the performance of the following 4 algorithms:\n","\n","- Decision Trees\n","- Bagging + Decision Trees\n","- Random Forest\n","- Extra Trees\n","\n","Note that in order for our results to be consistent, we have to expose the models to exactly the same CrossValidation scheme. Let's start by initializing that. Then, we'll initialize the models.\n","\n","*You can also create a function to speed up your work...*"]},{"metadata":{"colab_type":"code","id":"1WbzJM4cYp1I","outputId":"28629f94-449c-472e-827e-8c4d54006f79","executionInfo":{"status":"ok","timestamp":1551160949471,"user_tz":480,"elapsed":51988,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#1 Code to read csv file into colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 17.7MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.8MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 2.2MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.6MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 3.4MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.8MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.9MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.9MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 4.1MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 4.1MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 7.6MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 7.7MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 7.7MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 7.6MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 7.6MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 7.6MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 38.3MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 8.8MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 8.7MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 8.8MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 8.8MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 8.8MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 8.5MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 8.8MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 8.8MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 8.7MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 8.8MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 44.6MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 47.8MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 49.3MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 43.3MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 43.7MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 51.6MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 52.3MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 53.1MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 10.2MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 9.9MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 9.9MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 9.9MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 9.9MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 9.9MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 9.9MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 9.9MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 9.9MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 9.9MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 47.8MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 48.4MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 49.2MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 50.2MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 9.1MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 9.2MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 9.2MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 9.1MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 9.1MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 9.1MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 9.1MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 9.2MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 9.2MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 9.2MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 51.0MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 54.6MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 41.8MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 42.1MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 42.7MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 43.2MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 43.7MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 44.0MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 44.9MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 44.7MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 45.0MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 45.1MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 61.4MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 62.5MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 60.8MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 60.0MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 60.3MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 59.9MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 59.6MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 59.0MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 59.2MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 49.5MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 48.4MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 49.4MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 50.4MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 50.5MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 50.9MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 51.2MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 51.1MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 51.7MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 51.5MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 62.9MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 65.3MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 63.9MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 16.7MB/s \n","\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n","\u001b[?25h"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"ubjxW9IrYrMG","colab":{}},"cell_type":"code","source":["#2. Get the file\n","#make sure you upload all your data files to your Google drive and change share->Advanced->change->anyone with the link can view\n","downloaded = drive.CreateFile({'id':'1LFfAsD23aNrgYdwGCAumbyIYzu2F4Shz'}) # replace the id with id of file you want to access\n","downloaded.GetContentFile('car.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"dD3iO7R01vUp","colab_type":"code","outputId":"140e280a-b56d-4d0d-9537-2667a4a8802f","executionInfo":{"status":"ok","timestamp":1551160950094,"user_tz":480,"elapsed":52599,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('car.csv')\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>buying</th>\n","      <th>maint</th>\n","      <th>doors</th>\n","      <th>persons</th>\n","      <th>lug_boot</th>\n","      <th>safety</th>\n","      <th>acceptability</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>small</td>\n","      <td>low</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>small</td>\n","      <td>med</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>small</td>\n","      <td>high</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>med</td>\n","      <td>low</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>med</td>\n","      <td>med</td>\n","      <td>unacc</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  buying  maint doors persons lug_boot safety acceptability\n","0  vhigh  vhigh     2       2    small    low         unacc\n","1  vhigh  vhigh     2       2    small    med         unacc\n","2  vhigh  vhigh     2       2    small   high         unacc\n","3  vhigh  vhigh     2       2      med    low         unacc\n","4  vhigh  vhigh     2       2      med    med         unacc"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"8srbYBc41vUs","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","y = LabelEncoder().fit_transform(df['acceptability'])\n","X = pd.get_dummies(df.drop('acceptability', axis=1))"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"XAb_lP541vUt","colab_type":"code","outputId":"f93af51e-3b36-44c8-b2f1-e5b07076208f","executionInfo":{"status":"ok","timestamp":1551160950398,"user_tz":480,"elapsed":52896,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["y"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 2, ..., 2, 1, 3])"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"AwxWUD5z1vUw","colab_type":"code","outputId":"ed7e83c7-4933-4881-c613-b89466021524","executionInfo":{"status":"ok","timestamp":1551160951116,"user_tz":480,"elapsed":53611,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["#Find out how many different values and how often they appear in y\n","pd.Series(y).value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    1210\n","0     384\n","1      69\n","3      65\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"1oCYreTi1vUy","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n","\n","cv = StratifiedKFold(n_splits=3, shuffle=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oNQK2jZ-1vU0","colab_type":"code","outputId":"1c840d6b-3bc4-47cb-a7cb-2caccbdef47b","executionInfo":{"status":"ok","timestamp":1551160953374,"user_tz":480,"elapsed":55862,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["dt = DecisionTreeClassifier()\n","s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Decision Tree Score:\t0.956 ± 0.002\n"],"name":"stdout"}]},{"metadata":{"id":"-sqq7LmJ1vU2","colab_type":"code","outputId":"9d8d93c0-42f9-4334-89f1-c649b5418bd4","executionInfo":{"status":"ok","timestamp":1551160953384,"user_tz":480,"elapsed":55868,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#Repeat for BaggingClassifier()\n","dt = BaggingClassifier()\n","s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Decision Tree Score:\t0.97 ± 0.005\n"],"name":"stdout"}]},{"metadata":{"id":"uv1lBRRQ1vU4","colab_type":"code","outputId":"499bd8ee-e916-4c1d-99a6-784365edff8c","executionInfo":{"status":"ok","timestamp":1551160953387,"user_tz":480,"elapsed":55865,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#Repeat for RandomForestClassifier()\n","dt = RandomForestClassifier()\n","s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Decision Tree Score:\t0.929 ± 0.006\n"],"name":"stdout"}]},{"metadata":{"id":"-F26KMfq1vU6","colab_type":"code","outputId":"2a3e3e04-f4f8-4866-b550-061b13814abd","executionInfo":{"status":"ok","timestamp":1551160953391,"user_tz":480,"elapsed":55865,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#Repeat for ExtraTreesClassifier()\n","dt = ExtraTreesClassifier()\n","s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Decision Tree Score:\t0.953 ± 0.005\n"],"name":"stdout"}]},{"metadata":{"id":"lEZkVz4w1vU9","colab_type":"text"},"cell_type":"markdown","source":["Let's explore the effect of balancing our classes on the score."]},{"metadata":{"id":"hjsGzoI-1vU-","colab_type":"text"},"cell_type":"markdown","source":["## Conclusion\n","\n","We can improve the performance of a single model by generating multiple models and aggregating their predictions. \n","- If our $Y$ is continuous, we average each prediction. \n","- If our $Y$ is discrete, we use a \"plurality vote\" to decide the predicted value.\n","\n","The three types of ensemble models we discussed today:\n","1. **Bagged Decision Trees** are where we take the original data of size $n$, bootstrap $k$ samples each of size $n$ (with replacement!), build a decision tree on each sample, then make predictions by passing a test observation through all $k$ trees and developing one aggregate prediction for that observation.\n","2. **Random Forests** are where we bag decision trees, but when it comes to building each individual decision tree, we only consider a random subset of features at each split.\n","3. **ExtraTrees** are where we build random forests, but when it comes to building each individual decision tree, we also select a random split of each feature at each node.\n","\n","Some of these methods will perform better in some cases, some better in other cases. For example, decision trees are more nimble and easier to communicate, but have a tendency to overfit. On the other hand, ensemble methods perform better in more complex scenarios, but may become very complicated and harder to explain."]},{"metadata":{"id":"7X-KHWOb1vU_","colab_type":"text"},"cell_type":"markdown","source":["### ADDITIONAL RESOURCES\n","\n","- [Random Forest on wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n","- [Quora question on Random Forest](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n","- [Scikit Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n","- [Scikit Learn Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n","- [Ensemble Methods Paper](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)\n","\n","---\n","#### Boosting\n","- [Academic Introduction to Adaptive Boosting](http://rob.schapire.net/papers/explaining-adaboost.pdf)\n","- [Stack Exchange AdaBoost vs. Gradient Boosting](http://stats.stackexchange.com/questions/164233/intuitive-explanations-of-differences-between-gradient-boosting-trees-gbm-ad)\n","- [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n","- [Quora on Intuitive Explanations of AdaBoost](https://www.quora.com/What-is-AdaBoost)\n","- MIT on [Adaptive Boosting](http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf)\n","- A [Walk Through](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) on Tuning Gradient Boosting Models"]},{"metadata":{"id":"1oc4PfvNtrMi","colab_type":"text"},"cell_type":"markdown","source":[""]}]}