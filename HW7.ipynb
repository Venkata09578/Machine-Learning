{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW7Koouri-Prathusha.ipynb","version":"0.3.2","provenance":[{"file_id":"1NgeqcwdZXoqhvQ924KbJuIUb9YQep8mI","timestamp":1553477992625}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J-cKsCZ6axWv","colab_type":"text"},"source":["# HW #7 Neural Networks\n","\n","In this HW, we'll be exploring a visual proof of the universal approximation theorem and building (from scratch) a neural network that will approximate a pretty ridiculous function.\n","\n","Head over to [this site](http://neuralnetworksanddeeplearning.com/chap4.html) and read from the beginning of the page until the \"Many Input Variables\" section. (You do not need to read the \"Many Input Variables\" section and beyond but are certainly welcome to do so!) You'll read the introduction, the \"Two Caveats\" section, and the \"Universality with One Input and One Output\" section.\n","\n","Your answers to problems 1-5 should come from directly this reading."]},{"cell_type":"markdown","metadata":{"id":"uB_nGvAvaxW2","colab_type":"text"},"source":["**Problem 1**: Summarize the Universal Approximation Theorem. (Don't copy it; use your own words!)"]},{"cell_type":"markdown","metadata":{"id":"tgzYTOfwToEM","colab_type":"text"},"source":["### 1. As in the digital world NAND gates are said to be universal approximators as they can compute any function by adjusting weights. <br/>\n","### 2. Neural network with one hidden layer can be used to approximate any continuous function by adjusting the no. of neurons used in that layer (the get the best approximation we can use 2n+1 neurons in hidden layer , where n is no. of inputs) to any desired accuracy.<br/>\n","### 3. And as said by the author the combination of powerful algorithms for learning functions and universality(to learn anything by adjusting the parameters like neurons, etc.) neural networks can be tuned for any arbitrary input and still give output with very little error.\n"]},{"cell_type":"markdown","metadata":{"id":"ICxqzmDIaxW_","colab_type":"text"},"source":["**Problem 2**: Summarize the two caveats the author uses to describe the statement \"a neural network can compute any function.\""]},{"cell_type":"markdown","metadata":{"id":"TYMrDoIMXdL3","colab_type":"text"},"source":["### The 2 caveats are:<br/>\n","###  1. Universal approximator means it can only approximate the function's output but doesn't compute the exactly same function output without any error. As we increase the number of neurons used in the layer and increase the number of training examples, the neural network can improve on approximation made for the function's output. <br/>\n","###  2. The neural network can predict approximate output's for continuous function's , it cannot perform as per expectations on discrete or discontinuous functions, As neural network cannot capture sudden changes in the function. "]},{"cell_type":"markdown","metadata":{"id":"lx1kc_QEaxXJ","colab_type":"text"},"source":["**Problem 3:** For a sigmoidal activation function to closely resemble a step function, how would you describe the value of $w$? What constraints exist on the value of $b$? How do we calculate $s$? What does the value of $s$ indicate?\n","\n","Try playing around with the applets on [this site](http://neuralnetworksanddeeplearning.com/chap4.html) to test how different parts of the perceptron affect the output. This should be helpful in answering the questions above."]},{"cell_type":"markdown","metadata":{"id":"abu9xqQU6RuY","colab_type":"text"},"source":["### 1. The value of w should be set to very high eg:- 1000. \n","### For the function((1/(1+e^-x)) - sigmoid function), As Limit x -> 0 function value is equal to 1, Limit x -> 1 function value is equal to 0, where x = w*input + b.\n","### 2. As the bias(b) increases the graph moves to the left and as the bias decreases the graph moves to the right without any effect in shape , as bias is independent of 'x' value. By adjusting the bias we can adjust the starting position of step function.<br/>\n","### As per my understanding, when 'w' is a larger number , b value should be a negative number to see a step function , and by adjusting the value of 'b' we can adjust the step starting point.\n","### 3. s is directly proportional to bias and inversely proportional to weight.\n","### We calculate 's' by substituting the values of 'b' and 'w' in the following formula, \n","### s = -bias/weight.\n","### 4. The value of 's' indicate the position of step.\n"]},{"cell_type":"markdown","metadata":{"id":"XPyS6AeOaxXP","colab_type":"text"},"source":["**Problem 4**: When the author wants us to approximate $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ with a neural network, the function on the applet where we manipulate the values of $h_i$ is not $f(x)$. It's a different function. What is this function, and where on the feedforward Neural Network is can the vaule of this function be collected?"]},{"cell_type":"markdown","metadata":{"id":"ffQAI3GeCQ0w","colab_type":"text"},"source":["### The function is the inverse function calculated by inversing the sigmoid function and multiplied with actual function. The weighted output of the hidden layer should correspond to that value.\n","### The function is: $\\sigma^{-1}\\circ f(x)$\n","### As per my understanding, if we pass the above calculated function value to the final sigmoid gate , then the sigmoid and it's inverse cancels out and we will be left with the function's output which is close approximate to f(x). As we are trying to minimize the average deviation between the goal function and the actual function.\n","\n","### Where on the network is value collected? It is the weighted output of the hidden layer.\n"]},{"cell_type":"markdown","metadata":{"id":"6Ws174ZVaxXY","colab_type":"text"},"source":["**Problem 5**: The author asks you to find values of $h_i$ that make your neural network closely approximate $\\sigma^{-1}\\circ f(x)$. Record your values of $h_i$ here and your best \"average deviation\" score. Let's name the $h$s from top to bottom on the graph as $h_1$, $h_2$ ,,,, $h_5$,"]},{"cell_type":"markdown","metadata":{"id":"fsgRSVNgDHUv","colab_type":"text"},"source":["### h1 = -1.2, \n","### h2 = -1.4, \n","### h3= -0.3, \n","### h4=-0.9, \n","### h5=1.3 , \n","### Best Average Deviation = 0.38"]},{"cell_type":"markdown","metadata":{"id":"IbLaLSKRaxXf","colab_type":"text"},"source":["**Problem 6**: Build the neural network from your work in Problem 5 here with MLPClassifier.\n","\n","A few things to keep in mind:\n","* How many inputs are there? \n","* How many outputs are there?\n","* How many neurons are in the hidden layer?\n"]},{"cell_type":"code","metadata":{"id":"bZWM9fYAaxXg","colab_type":"code","colab":{}},"source":["import warnings; warnings.simplefilter('ignore')\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","from sklearn.neural_network import MLPClassifier, MLPRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qBlT04wBEAXt","colab_type":"code","colab":{}},"source":["X = pd.DataFrame(np.linspace(0,1,1000))\n","y = pd.DataFrame(0.2 + 0.4 * X**2 + 0.3 * X * np.sin(15*X) + 0.05 * np.cos(50*X))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANfNwwnuIwjx","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eB0IXVh2K1eG","colab_type":"code","outputId":"1e035f72-2391-4d65-988c-9e8032f5092d","executionInfo":{"status":"ok","timestamp":1553705794464,"user_tz":420,"elapsed":1631,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# activation = relu, solver function = adam\n","mlp = MLPRegressor(hidden_layer_sizes=10, learning_rate_init=0.09, random_state=0)\n","mlp.fit(X_train,y_train)\n","accuracy = mlp.score(X_test,y_test)\n","y_hat = mlp.predict(X)\n","MSE = mean_squared_error(y, y_hat)\n","print(\"Accuracy: {}, Mean Squared Error: {}\".format(accuracy,MSE))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy: 0.8330016728356665, Mean Squared Error: 0.008206003447028449\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E4VwpHTkaxXo","colab_type":"text"},"source":["**Problem 7**: Once you've built the neural network with the structure described in Problem 6, use `np.linspace` to generate 1000 values of $x$ between 0 and 1 and estimate the performance of your neural network using mean squared error.\n","\n","Recall that mean squared error(MSE) is given by:\n","\n","$$\n","\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}-y)^2\n","$$\n","\n","\n","* Your $\\hat{y}$ in this case are your predicted values from your neural network for each of the $x$ that you generated using `np.linspace`. Make sure to take into account the final activation function!\n","* Your $y$ values are the actual observed values of $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ for each of the $x$ that you generated using `np.linspace`.\n","* Use  [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) for this work.\n","* Plot  $f(x)$ curve overlapped with the curve predicted by your trained Neural Networks for x between 0 and 1\n","* Try different learning rates"]},{"cell_type":"code","metadata":{"id":"eXJ2Tug7Oied","colab_type":"code","outputId":"ed66dd0c-4d5a-4037-ac80-150f4896abf4","executionInfo":{"status":"ok","timestamp":1553705794466,"user_tz":420,"elapsed":1619,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["y_hat = mlp.predict(X)\n","\n","MSE = mean_squared_error(y,y_hat)\n","print(MSE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.008206003447028449\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jXv29gDVRfVv","colab_type":"code","colab":{}},"source":["def plot_hyper_param_plots(x, y, y_hat, xlabel):\n","    plt.plot(x, y, label='actual')\n","    plt.plot(x, y_hat, label='predicted')\n","    plt.xlabel(xlabel)\n","    plt.ylabel('Test Accuracy')\n","    plt.legend(loc='upper left')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGydaetNSfmp","colab_type":"code","outputId":"2e24ae36-668e-40e1-e9b4-c5beec8a7fc7","executionInfo":{"status":"ok","timestamp":1553705794470,"user_tz":420,"elapsed":1600,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["plot_hyper_param_plots(X, y, y_hat, \"X\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCGEn5IEIIQQfur/A0gWbzA/glSUAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"T8f5z-GlWGTl","colab_type":"code","outputId":"f24cd533-b1e0-4cb1-d313-295359533f06","executionInfo":{"status":"ok","timestamp":1553705796524,"user_tz":420,"elapsed":3637,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["MSE_list = {}\n","for alpha in np.arange(0.001,0.3,0.01):\n","  mlp = MLPRegressor(hidden_layer_sizes=10,activation='relu', learning_rate_init=alpha, random_state=0)\n","  mlp.fit(X_train,y_train)\n","  accuracy = mlp.score(X_test,y_test)\n","  y_hat = mlp.predict(X)\n","  MSE = mean_squared_error(y, y_hat)\n","  MSE_list[alpha]= MSE\n","  print(\"The learning rate is : {} and the MSE is : {}\".format(round(alpha,3) , round(MSE,4)))\n","print()\n","print(\" The minimum value for MSE is at learning rate: \", min(MSE_list.items(), key=lambda x: x[1]) )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The learning rate is : 0.001 and the MSE is : 0.0206\n","The learning rate is : 0.011 and the MSE is : 0.0192\n","The learning rate is : 0.021 and the MSE is : 0.0191\n","The learning rate is : 0.031 and the MSE is : 0.019\n","The learning rate is : 0.041 and the MSE is : 0.0187\n","The learning rate is : 0.051 and the MSE is : 0.0188\n","The learning rate is : 0.061 and the MSE is : 0.0183\n","The learning rate is : 0.071 and the MSE is : 0.0083\n","The learning rate is : 0.081 and the MSE is : 0.0084\n","The learning rate is : 0.091 and the MSE is : 0.0082\n","The learning rate is : 0.101 and the MSE is : 0.0084\n","The learning rate is : 0.111 and the MSE is : 0.0083\n","The learning rate is : 0.121 and the MSE is : 0.0086\n","The learning rate is : 0.131 and the MSE is : 0.0178\n","The learning rate is : 0.141 and the MSE is : 0.0201\n","The learning rate is : 0.151 and the MSE is : 0.0201\n","The learning rate is : 0.161 and the MSE is : 0.0201\n","The learning rate is : 0.171 and the MSE is : 0.02\n","The learning rate is : 0.181 and the MSE is : 0.02\n","The learning rate is : 0.191 and the MSE is : 0.0201\n","The learning rate is : 0.201 and the MSE is : 0.0201\n","The learning rate is : 0.211 and the MSE is : 0.02\n","The learning rate is : 0.221 and the MSE is : 0.02\n","The learning rate is : 0.231 and the MSE is : 0.02\n","The learning rate is : 0.241 and the MSE is : 0.0196\n","The learning rate is : 0.251 and the MSE is : 0.0169\n","The learning rate is : 0.261 and the MSE is : 0.0169\n","The learning rate is : 0.271 and the MSE is : 0.017\n","The learning rate is : 0.281 and the MSE is : 0.0171\n","The learning rate is : 0.291 and the MSE is : 0.0171\n","\n"," The minimum value for MSE is at learning rate:  (0.09099999999999998, 0.008219854292480427)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8J8JsBvqaxX0","colab_type":"text"},"source":["**Problem 8**: Suppose you wanted to increase the performance of this neural network. How might you go about doing so?\n","* You try different number of hidden layers and different number of neurons in each hidden layer.\n","* Report your best Neural Network structure and the minimum MSE you can get."]},{"cell_type":"code","metadata":{"id":"zaItTkTfOjp-","colab_type":"code","colab":{}},"source":["# functions to test different hidden layers and learning rates\n","def train_mlp(units, activation='relu', lr=0.07, batch_size=200,loss_curve=False):\n","    \n","    mlp = MLPRegressor(hidden_layer_sizes=units, activation=activation, batch_size=batch_size, momentum=0.0, learning_rate_init=lr, random_state=0)\n","    \n","    mlp.fit(X_train, y_train)\n","    accuracy = mlp.score(X_test,y_test)\n","# #     print(mlp.get_params())\n","#     if loss_curve:\n","#         return mlp.loss_curve_, accuracy\n","    \n","    y_hat = mlp.predict(X)\n","\n","    MSE = mean_squared_error(y,y_hat)\n","    \n","    return MSE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXMxrBtkRBpk","colab_type":"code","outputId":"3ae01464-0715-448b-bbcc-5571d065970e","executionInfo":{"status":"ok","timestamp":1553705798304,"user_tz":420,"elapsed":5396,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":381}},"source":["for units in range(5,110,5):\n","  print(\"No. of neurons in one hidden layer are: {} and MSE is {}\".format(units, round(train_mlp(units),4)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["No. of neurons in one hidden layer are: 5 and MSE is 0.0086\n","No. of neurons in one hidden layer are: 10 and MSE is 0.0083\n","No. of neurons in one hidden layer are: 15 and MSE is 0.0082\n","No. of neurons in one hidden layer are: 20 and MSE is 0.0195\n","No. of neurons in one hidden layer are: 25 and MSE is 0.02\n","No. of neurons in one hidden layer are: 30 and MSE is 0.018\n","No. of neurons in one hidden layer are: 35 and MSE is 0.0172\n","No. of neurons in one hidden layer are: 40 and MSE is 0.0082\n","No. of neurons in one hidden layer are: 45 and MSE is 0.0082\n","No. of neurons in one hidden layer are: 50 and MSE is 0.0082\n","No. of neurons in one hidden layer are: 55 and MSE is 0.0078\n","No. of neurons in one hidden layer are: 60 and MSE is 0.0166\n","No. of neurons in one hidden layer are: 65 and MSE is 0.0165\n","No. of neurons in one hidden layer are: 70 and MSE is 0.017\n","No. of neurons in one hidden layer are: 75 and MSE is 0.0082\n","No. of neurons in one hidden layer are: 80 and MSE is 0.0168\n","No. of neurons in one hidden layer are: 85 and MSE is 0.0083\n","No. of neurons in one hidden layer are: 90 and MSE is 0.0198\n","No. of neurons in one hidden layer are: 95 and MSE is 0.0131\n","No. of neurons in one hidden layer are: 100 and MSE is 0.0081\n","No. of neurons in one hidden layer are: 105 and MSE is 0.0104\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DlfZYBWVRZDE","colab_type":"code","outputId":"79b11311-a74a-43e3-e819-42e5a84cdb4f","executionInfo":{"status":"ok","timestamp":1553705800095,"user_tz":420,"elapsed":7176,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["import itertools\n","for i,j in itertools.product(range(10,50,10), range(10,50,10)):\n","  units = i,j\n","  print(\"Number of Neurons: {} and MSE is {}\".format(units,train_mlp(units)))\n","  \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of Neurons: (10, 10) and MSE is 0.008131482417741687\n","Number of Neurons: (10, 20) and MSE is 0.008283015919445717\n","Number of Neurons: (10, 30) and MSE is 0.016819721803925207\n","Number of Neurons: (10, 40) and MSE is 0.016886932147283488\n","Number of Neurons: (20, 10) and MSE is 0.016584317063646187\n","Number of Neurons: (20, 20) and MSE is 0.01763867552836577\n","Number of Neurons: (20, 30) and MSE is 0.004002697053850932\n","Number of Neurons: (20, 40) and MSE is 0.016944640914849967\n","Number of Neurons: (30, 10) and MSE is 0.016767207693087465\n","Number of Neurons: (30, 20) and MSE is 0.01735054727147362\n","Number of Neurons: (30, 30) and MSE is 0.003337015963686106\n","Number of Neurons: (30, 40) and MSE is 0.017030044328483672\n","Number of Neurons: (40, 10) and MSE is 0.016797788746990883\n","Number of Neurons: (40, 20) and MSE is 0.0025675610626119975\n","Number of Neurons: (40, 30) and MSE is 0.007786395985114117\n","Number of Neurons: (40, 40) and MSE is 0.016883874757271575\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aLDMQNM4XjNc","colab_type":"code","outputId":"389acdd1-9d79-48e0-b671-7ad78a15d460","executionInfo":{"status":"ok","timestamp":1553705801998,"user_tz":420,"elapsed":9068,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["num_hidden_units = range(5, 105, 5)\n","all_test_accuracy = []\n","for units in num_hidden_units: \n","    MSE = train_mlp((units,))\n","    print (\"Units: {}, MSE: {}\".format(units, MSE))\n","    all_test_accuracy.append(MSE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Units: 5, MSE: 0.008627715659812643\n","Units: 10, MSE: 0.008268088815815477\n","Units: 15, MSE: 0.008189517277398129\n","Units: 20, MSE: 0.019498840904076675\n","Units: 25, MSE: 0.020010487789750203\n","Units: 30, MSE: 0.01797301249956988\n","Units: 35, MSE: 0.017168688433507256\n","Units: 40, MSE: 0.00820602001227029\n","Units: 45, MSE: 0.008214649687012417\n","Units: 50, MSE: 0.008225787447225479\n","Units: 55, MSE: 0.007786107924159358\n","Units: 60, MSE: 0.01664534459149529\n","Units: 65, MSE: 0.016495153485915445\n","Units: 70, MSE: 0.017040012341672038\n","Units: 75, MSE: 0.008181800910463959\n","Units: 80, MSE: 0.016762002324302163\n","Units: 85, MSE: 0.008333458111615922\n","Units: 90, MSE: 0.01982755507601791\n","Units: 95, MSE: 0.013100597030426667\n","Units: 100, MSE: 0.008135555514288764\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"px1Smo6XTnY1","colab_type":"code","outputId":"bd1ada0f-7d96-4d32-9182-527f313d4b7b","executionInfo":{"status":"ok","timestamp":1553705806057,"user_tz":420,"elapsed":13116,"user":{"displayName":"Prathusha Koouri","photoUrl":"","userId":"11784553990265599358"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["best_units = num_hidden_units[np.argmin(all_test_accuracy)]\n","best_depths = []\n","\n","for depth in range(1, 11):\n","    MSE = train_mlp((best_units,)*depth)\n","    best_depths.append(MSE)\n","    print (\"No. of Neurons: {} , Depth of the Network: {}, Mean Squared Error: {}\".format(best_units,depth, MSE))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["No. of Neurons: 55 , Depth of the Network: 1, Mean Squared Error: 0.007786107924159358\n","No. of Neurons: 55 , Depth of the Network: 2, Mean Squared Error: 0.016790081745546085\n","No. of Neurons: 55 , Depth of the Network: 3, Mean Squared Error: 0.04105211948164741\n","No. of Neurons: 55 , Depth of the Network: 4, Mean Squared Error: 0.008109640328754043\n","No. of Neurons: 55 , Depth of the Network: 5, Mean Squared Error: 0.00411228761899631\n","No. of Neurons: 55 , Depth of the Network: 6, Mean Squared Error: 0.0410178005162985\n","No. of Neurons: 55 , Depth of the Network: 7, Mean Squared Error: 0.04104591355364151\n","No. of Neurons: 55 , Depth of the Network: 8, Mean Squared Error: 0.04134471184117376\n","No. of Neurons: 55 , Depth of the Network: 9, Mean Squared Error: 0.04506295757181517\n","No. of Neurons: 55 , Depth of the Network: 10, Mean Squared Error: 0.04556403052280324\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oB9ats322Wjv","colab_type":"text"},"source":["### I have tried different number of hidden layers and different number of neurons in each hidden layer.\n","### The best Neural Network structure and the minimum MSE :\n","\n","### Number of Neurons: (20, 30) and MSE is 0.004002697053850932\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"8WRchy39XR6T","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}